""" Moodle ZIP file multiprocessing supported parser."""

import zlib
from concurrent.futures import ProcessPoolExecutor, as_completed
from zipfile import ZipFile
from typing import List, Tuple, Dict
import pandas as pd

from parser_lark import parse_execution, TransformedData
from utils import generate_hash, chunkify


def process_batch(batch: List[Dict[str, str]]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Processes a batch of submissions to generate two Pandas dataframes: one for submissions and
    the other for point groups. Each entry in the batch is parsed, and information is
    extracted to create structured data for analysis.

    Args:
        batch: List of dictionaries where each dictionary represents a submission
            containing a `student_folder`, `execution_content` and `datetime`.

    Returns:
        A tuple of two incomplete Pandas dataframes:
        - The first dataframe contains processed submission information such as
          `student_id`, `submission_id`, `grade`, `timestamp`, and `runtime`.
        - The second dataframe contains group-specific details for each submission,
          including `submission_id`, `group_name`, `points`, `points_max`,
          `weighted_points`, and `weighted_points_max`.
    """
    batch_submissions: List[Dict[str, object]] = []
    batch_point_groups: List[Dict[str, object]] = []


    for item in batch:
        student_id = generate_hash(item["student_folder"])
        try:
            data: TransformedData = parse_execution(item["execution_file_content"])
        except Exception as e:
            print(f"Parse failed for student with id {student_id}: {e}")
            continue

        date_time_string_from_execution_folder = item["datetime"].strip('.ceg')
        pandas_datetime = pd.to_datetime(date_time_string_from_execution_folder, format='%Y-%m-%d-%H-%M-%S')

        hash_input_from_student_id = (student_id + str(int(data.timestamp.timestamp() * 1000))).encode()
        submission_id_octal = hex(zlib.crc32(hash_input_from_student_id))[-8:]

        batch_submissions.append({
            "student_id": student_id,
            "submission_id": submission_id_octal,
            "grade": data.grade,
            "timestamp": pandas_datetime,
            "execution_timestamp": data.timestamp,
            "runtime": data.timestamp,
        })

        for group in data.groups:
            batch_point_groups.append({
                "timestamp": data.timestamp,
                "submission_id": submission_id_octal,
                "group_name": group.group_name,
                "points": group.points,
                "points_max": group.points_max,
                "weighted_points": group.weighted_points,
                "weighted_points_max": group.weighted_points_max,
            })

    return pd.DataFrame(batch_submissions), pd.DataFrame(batch_point_groups)


def parse_homework(zip_path: str, batch_size: int = 70) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Generated by AI assistant.

    Parses a zip archive containing student homework execution files, processes them in
    batches using multiprocessing, and returns the resultant submissions and point groups
    as pandas DataFrames.

    The function reads the zip file to extract specific execution-related files
    (`execution.txt`) from student folders, chunks them into smaller batches, processes
    each batch in parallel, and aggregates the data into two categorized outputs.

    Args:
        zip_path: The file path to the zip archive containing homework execution files.
        batch_size: The number of files to be processed in each batch. Defaults to 10.

    Returns:
        Tuple[pd.DataFrame, pd.DataFrame]: A tuple of pandas DataFrames, where the first
        DataFrame contains the processed submissions and the second contains the point
        group data.
    """

    with ZipFile(zip_path, 'r') as archive:
        execution_files: list[dict[str, str]] = [
            {
                "student_folder": file.split('/')[0],
                "execution_file_content": archive.read(file).decode('utf-8'),
                "datetime": file.split('/')[1]
            }
            for file in archive.namelist()
            if len(file.split('/')) >= 2 and file.endswith('execution.txt')
        ]

    batched_files = list(chunkify(execution_files, batch_size))

    with ProcessPoolExecutor() as executor:
        futures = [executor.submit(process_batch, batch) for batch in batched_files]

        submissions_batches = []
        point_groups_batches = []

        for future in as_completed(futures):
            batch_submissions_df, batch_point_groups_df = future.result()
            submissions_batches.append(batch_submissions_df)
            point_groups_batches.append(batch_point_groups_df)

    submissions_df = pd.concat(submissions_batches, ignore_index=True)
    point_groups_df = pd.concat(point_groups_batches, ignore_index=True)

    return submissions_df, point_groups_df
